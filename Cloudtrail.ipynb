{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77539f-3b06-43ff-b231-97e00e86f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "\n",
    "import awswrangler\n",
    "import boto3\n",
    "from matplotlib import cycler\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import matplotlib.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f8d880-9806-4f5d-9fcb-c9891366217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure me!\n",
    "AWS_PROFILE = \"\"\n",
    "CLOUDTRAIL_ATHENA_TABLE_NAME = \"\"\n",
    "CLOUDTRAIL_ATHENA_WORKGROUP = \"\"\n",
    "CLOUDTRAIL_ATHENA_DATABASE = \"\"\n",
    "CLOUDTRAIL_ATHENA_CATALOG = \"\"\n",
    "DAYS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a8219-0b12-40e1-a84a-bf09ca0623ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session(profile_name=AWS_PROFILE)\n",
    "\n",
    "def execute_query(query):\n",
    "    return awswrangler.athena.read_sql_query(\n",
    "        query,\n",
    "        database=CLOUDTRAIL_ATHENA_DATABASE,\n",
    "        data_source=CLOUDTRAIL_ATHENA_CATALOG,\n",
    "        workgroup=CLOUDTRAIL_ATHENA_WORKGROUP,\n",
    "        athena_cache_settings={\"max_cache_seconds\": 60 * 60},\n",
    "        boto3_session=session,\n",
    "        ctas_approach=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201e14c-0aea-44e4-9856-8517035557ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_style():\n",
    "    background=\"#24283b\"\n",
    "    foreground=\"#c0caf5\"\n",
    "    comment=\"#565f89\"\n",
    "    cycle=[\n",
    "        \"#7aa2f7\",  # blue\n",
    "        \"#ff9e64\",  # orange\n",
    "        \"#9ece6a\",  # green\n",
    "        \"#f7768e\",  # red\n",
    "        \"#9d7cd8\",  # purple\n",
    "        \"#bb9af7\",  # magenta\n",
    "        \"#565f89\",  # comment\n",
    "        \"#e0af68\",  # yellow\n",
    "        \"#7dcfff\",  # cyan\n",
    "    ]\n",
    "\n",
    "    plt.style.use({\n",
    "        \"lines.color\": foreground,\n",
    "        \"patch.edgecolor\": foreground,\n",
    "        \"text.color\": foreground,\n",
    "        \"axes.facecolor\": background,\n",
    "        \"axes.edgecolor\": foreground,\n",
    "        \"axes.labelcolor\": foreground,\n",
    "        \"xtick.color\": foreground,\n",
    "        \"ytick.color\": foreground,\n",
    "        \"legend.framealpha\": 0,\n",
    "        \"grid.color\": comment,\n",
    "        \"figure.facecolor\": background,\n",
    "        \"figure.edgecolor\": background,\n",
    "        \"savefig.facecolor\": background,\n",
    "        \"savefig.edgecolor\": background,\n",
    "        \"boxplot.boxprops.color\": foreground,\n",
    "        \"boxplot.capprops.color\": foreground,\n",
    "        \"boxplot.flierprops.color\": foreground,\n",
    "        \"boxplot.flierprops.markeredgecolor\": foreground,\n",
    "        \"boxplot.whiskerprops.color\": foreground,\n",
    "        \"axes.prop_cycle\": cycler(color=cycle)\n",
    "    })\n",
    "\n",
    "init_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3efa88-5a3e-402c-ab80-5d7cb7195373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_latency(data, label_fn=None, title=\"CloudTrail delay statistics\"):\n",
    "    ticks = pandas.DataFrame(data={\"seconds\": [60, 300, 600, 3600, 86400], \"labels\": [\"1 min\", \"5 min\", \"10 min\", \"1 hour\", \"24 hours\"]})\n",
    "\n",
    "    max_cols = 2\n",
    "    n_rows = 1 + ((len(data) - 1) // max_cols)\n",
    "    n_cols = max_cols if len(data) > max_cols else len(data)\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 6 * n_rows), squeeze=False)\n",
    "    \n",
    "    fig.supylabel(\"Cumulative events delivered\")\n",
    "    fig.supxlabel(\"CloudTrail delay delivering to S3\")\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for ((index, row), ax) in zip(data.iterrows(), axs.flat):\n",
    "        hist_bins = json.loads(row[\"s3_write_delay_seconds_histogram\"].replace('{', '[').replace('}', ']'))\n",
    "        bins = np.array([each[0] for each in hist_bins])\n",
    "        counts = np.array([each[1] for each in hist_bins])\n",
    "    \n",
    "        # Go up to 24 hours, we don't control the buckets\n",
    "        if (bins[-1] < 24 * 60 * 60):\n",
    "            bins = np.append(bins, [24 * 60 * 60])\n",
    "            counts = np.append(counts, [0])\n",
    "    \n",
    "        # Plot cumulative\n",
    "        total = counts.sum()\n",
    "        counts = 100 * counts/total;\n",
    "    \n",
    "        ax.hist(bins, bins, weights=counts, cumulative=True)\n",
    "        if label_fn:\n",
    "            ax.set_title(label_fn(row))\n",
    "    \n",
    "        # Show linear scale up to 600 seconds, then log\n",
    "        ax.set_xscale(\"symlog\", linthresh=600)\n",
    "        \n",
    "        ax.set_xticks(ticks[\"seconds\"], ticks[\"labels\"])\n",
    "        ax.set_xticks([], [], minor=True)\n",
    "        ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "        \n",
    "        for tick in ticks[\"seconds\"]:\n",
    "            ax.axvline(x = tick, linestyle=\"dashed\", alpha=0.3)\n",
    "            \n",
    "        for tick in ticks[\"seconds\"][:-1]:\n",
    "            lt_ratio = row[f\"s3_write_delay_seconds_count_lt_{tick}\"] / total\n",
    "            if 1 > lt_ratio > 0.99:\n",
    "                label = f\"{lt_ratio:%}\"\n",
    "            else:\n",
    "                label = f\"{100 * lt_ratio:.2f}%\"\n",
    "        \n",
    "            plt.text(tick, 0.95, label, transform=transforms.offset_copy(ax.get_xaxis_transform(), fig, x=0.05))\n",
    "            if lt_ratio == 1:\n",
    "                break\n",
    "        \n",
    "    \n",
    "        ax.set_xmargin(0)\n",
    "        ax.set_ymargin(.1)\n",
    "\n",
    "        def interval_string(seconds):\n",
    "            return str(timedelta(seconds=round(seconds)))\n",
    "\n",
    "        table_cells = [\n",
    "            [stat, interval_string(row[f\"s3_write_delay_seconds_{stat}\"])]\n",
    "            for stat in (\"min\", \"avg\", \"p50\", \"p95\", \"p99\", \"max\")\n",
    "        ]\n",
    "            \n",
    "        table = ax.table(cellText=table_cells, bbox=[0.6,0.05,0.35,0.35], colWidths=[0.4, 0.6], zorder=1, edges=\"open\")\n",
    "        table.auto_set_font_size(False)\n",
    "        for cell in table.get_celld().values():\n",
    "            cell.set_text_props(color=\"white\", alpha=0.6, fontweight=\"bold\", fontsize=\"medium\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b79600-3fe3-4fd5-b854-96f98e33dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_events(data, title, xlabel, ylabel):\n",
    "    ticks = [0, 60, 120, 180, 240, 300]\n",
    "\n",
    "    max_cols = 2\n",
    "    n_rows = 1 + ((len(data) - 1) // max_cols)\n",
    "    n_cols = max_cols if len(data) > max_cols else len(data)\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 6 * n_rows), squeeze=False)\n",
    "    \n",
    "    fig.supxlabel(xlabel)\n",
    "    fig.supylabel(ylabel)\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    for (group, rows), ax in zip(data, axs.flat):\n",
    "        ax.scatter(rows.iloc[:, 0], rows.iloc[:, 1], s=0.1)\n",
    "        \n",
    "        ax.set_xticks(ticks)\n",
    "        ax.set_xticks([], [], minor=True)\n",
    "        ax.set_yticks(ticks)\n",
    "        ax.set_yticks([], [], minor=True)\n",
    "        if group:\n",
    "            ax.set_title(group)\n",
    "        ax.set_xlim(0, 300)\n",
    "        ax.set_ylim(0, 360)\n",
    "        \n",
    "        #for tick in ticks:\n",
    "        #     ax.axhline(y = tick, linestyle=\"dashed\", alpha=0.1)\n",
    "        \n",
    "    \n",
    "        ax.set_xmargin(0)\n",
    "        ax.set_ymargin(0)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257aa5fa-ada8-42e4-9657-076184168a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_events = execute_query(f\"\"\"\n",
    "with events as (\n",
    "    select\n",
    "        *,\n",
    "        \"$path\",\n",
    "        parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as parsed_eventtime,\n",
    "        \"$file_modified_time\",\n",
    "        \"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as s3_write_delay_interval,\n",
    "        to_milliseconds(\"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ')) / 1000 as s3_write_delay_seconds,\n",
    "        regexp_extract(\"$path\", '\\d{8}T\\d{4}') as path_timestamp,\n",
    "       \"$file_size\"\n",
    "    from\n",
    "        \"{CLOUDTRAIL_ATHENA_TABLE_NAME}\"\n",
    "    where\n",
    "        addendum is null\n",
    "    and eventtype = 'AwsApiCall'\n",
    ")\n",
    "select\n",
    "    count(1) as \"count\",\n",
    "    min(s3_write_delay_seconds) as \"s3_write_delay_seconds_min\",\n",
    "    avg(s3_write_delay_seconds) as \"s3_write_delay_seconds_avg\",\n",
    "    map_entries(numeric_histogram(100, s3_write_delay_seconds)) as \"s3_write_delay_seconds_histogram\",\n",
    "    avg(\"$file_size\") as size,\n",
    "    approx_percentile(s3_write_delay_seconds, 0.5) as \"s3_write_delay_seconds_p50\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.95) as \"s3_write_delay_seconds_p95\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.99) as \"s3_write_delay_seconds_p99\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.999) as \"s3_write_delay_seconds_p999\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.9999) as \"s3_write_delay_seconds_p9999\",\n",
    "    max(s3_write_delay_seconds) as \"s3_write_delay_seconds_max\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 60) as \"s3_write_delay_seconds_count_lt_60\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 300) as \"s3_write_delay_seconds_count_lt_300\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 600) as \"s3_write_delay_seconds_count_lt_600\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 3600) as \"s3_write_delay_seconds_count_lt_3600\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 86400) as \"s3_write_delay_seconds_count_lt_86400\"\n",
    "from\n",
    "    events\n",
    "where\n",
    "    eventdate between\n",
    "            format_datetime(current_date - interval '{1 + DAYS}' day, 'YYYY/MM/dd')\n",
    "        and format_datetime(current_date - interval '1' day, 'YYYY/MM/dd')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f260e3-a8de-488c-84ee-aeb7b79758ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_latency(data_all_events).savefig(\"all_events.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511a366-07af-4150-8458-82c6f0ed059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_managementevent = execute_query(f\"\"\"\n",
    "with events as (\n",
    "    select\n",
    "        *,\n",
    "        \"$path\",\n",
    "        parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as parsed_eventtime,\n",
    "        \"$file_modified_time\",\n",
    "        \"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as s3_write_delay_interval,\n",
    "        to_milliseconds(\"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ')) / 1000 as s3_write_delay_seconds,\n",
    "        regexp_extract(\"$path\", '\\d{8}T\\d{4}') as path_timestamp,\n",
    "       \"$file_size\"\n",
    "    from\n",
    "        \"{CLOUDTRAIL_ATHENA_TABLE_NAME}\"\n",
    "    where\n",
    "        addendum is null\n",
    "    and eventtype = 'AwsApiCall'\n",
    ")\n",
    "select\n",
    "    managementevent,\n",
    "    count(1) as \"count\",\n",
    "    min(s3_write_delay_seconds) as \"s3_write_delay_seconds_min\",\n",
    "    avg(s3_write_delay_seconds) as \"s3_write_delay_seconds_avg\",\n",
    "    map_entries(numeric_histogram(100, s3_write_delay_seconds)) as \"s3_write_delay_seconds_histogram\",\n",
    "    avg(\"$file_size\") as size,\n",
    "    approx_percentile(s3_write_delay_seconds, 0.5) as \"s3_write_delay_seconds_p50\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.95) as \"s3_write_delay_seconds_p95\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.99) as \"s3_write_delay_seconds_p99\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.999) as \"s3_write_delay_seconds_p999\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.9999) as \"s3_write_delay_seconds_p9999\",\n",
    "    max(s3_write_delay_seconds) as \"s3_write_delay_seconds_max\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 60) as \"s3_write_delay_seconds_count_lt_60\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 300) as \"s3_write_delay_seconds_count_lt_300\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 600) as \"s3_write_delay_seconds_count_lt_600\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 3600) as \"s3_write_delay_seconds_count_lt_3600\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 86400) as \"s3_write_delay_seconds_count_lt_86400\"\n",
    "from\n",
    "    events\n",
    "where\n",
    "    eventdate between\n",
    "            format_datetime(current_date - interval '{1 + DAYS}' day, 'YYYY/MM/dd')\n",
    "        and format_datetime(current_date - interval '1' day, 'YYYY/MM/dd')\n",
    "    and eventsource in (\n",
    "        's3.amazonaws.com'\n",
    "    )\n",
    "group by\n",
    "    managementevent\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d7ba6-e11c-4ad6-948f-ff68f4153297",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_latency(\n",
    "    data_by_managementevent,\n",
    "    label_fn=lambda row: \"Management events\" if row[\"managementevent\"] else \"Data events\",\n",
    "    title=\"CloudTrail delay statistics (S3 events)\",\n",
    ").savefig(\"s3_events_by_management.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359876c-e31d-4d8e-a82c-a53d1e1e3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_eventsource = execute_query(f\"\"\"\n",
    "with events as (\n",
    "    select\n",
    "        *,\n",
    "        \"$path\",\n",
    "        parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as parsed_eventtime,\n",
    "        \"$file_modified_time\",\n",
    "        \"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as s3_write_delay_interval,\n",
    "        to_milliseconds(\"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ')) / 1000 as s3_write_delay_seconds,\n",
    "        regexp_extract(\"$path\", '\\d{8}T\\d{4}') as path_timestamp,\n",
    "       \"$file_size\"\n",
    "    from\n",
    "        \"{CLOUDTRAIL_ATHENA_TABLE_NAME}\"\n",
    "    where\n",
    "        addendum is null\n",
    "    and eventtype = 'AwsApiCall'\n",
    ")\n",
    "select\n",
    "    eventsource,\n",
    "    count(1) as \"count\",\n",
    "    min(s3_write_delay_seconds) as \"s3_write_delay_seconds_min\",\n",
    "    avg(s3_write_delay_seconds) as \"s3_write_delay_seconds_avg\",\n",
    "    map_entries(numeric_histogram(100, s3_write_delay_seconds)) as \"s3_write_delay_seconds_histogram\",\n",
    "    avg(\"$file_size\") as size,\n",
    "    approx_percentile(s3_write_delay_seconds, 0.5) as \"s3_write_delay_seconds_p50\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.95) as \"s3_write_delay_seconds_p95\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.99) as \"s3_write_delay_seconds_p99\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.999) as \"s3_write_delay_seconds_p999\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.9999) as \"s3_write_delay_seconds_p9999\",\n",
    "    max(s3_write_delay_seconds) as \"s3_write_delay_seconds_max\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 60) as \"s3_write_delay_seconds_count_lt_60\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 300) as \"s3_write_delay_seconds_count_lt_300\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 600) as \"s3_write_delay_seconds_count_lt_600\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 3600) as \"s3_write_delay_seconds_count_lt_3600\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 86400) as \"s3_write_delay_seconds_count_lt_86400\"\n",
    "from\n",
    "    events\n",
    "where\n",
    "    eventdate between\n",
    "            format_datetime(current_date - interval '{1 + DAYS}' day, 'YYYY/MM/dd')\n",
    "        and format_datetime(current_date - interval '1' day, 'YYYY/MM/dd')\n",
    "    and eventsource in (\n",
    "        's3.amazonaws.com',\n",
    "        'sts.amazonaws.com',\n",
    "        'ssm.amazonaws.com',\n",
    "        'ec2.amazonaws.com'\n",
    "    )\n",
    "group by\n",
    "    eventsource\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6e2ee-a104-4cd4-80d5-0dd560d550de",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_latency(\n",
    "    data_by_eventsource,\n",
    "    label_fn=lambda row: row[\"eventsource\"],\n",
    "    title=\"CloudTrail delay statistics by event source\",\n",
    ").savefig(\"all_events_by_source.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab8700-c927-472b-8370-662f1f8394a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_eventname = execute_query(f\"\"\"\n",
    "with events as (\n",
    "    select\n",
    "        *,\n",
    "        \"$path\",\n",
    "        parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as parsed_eventtime,\n",
    "        \"$file_modified_time\",\n",
    "        \"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as s3_write_delay_interval,\n",
    "        to_milliseconds(\"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ')) / 1000 as s3_write_delay_seconds,\n",
    "        regexp_extract(\"$path\", '\\d{8}T\\d{4}') as path_timestamp,\n",
    "       \"$file_size\"\n",
    "    from\n",
    "        \"{CLOUDTRAIL_ATHENA_TABLE_NAME}\"\n",
    "    where\n",
    "        addendum is null\n",
    "    and eventtype = 'AwsApiCall'\n",
    ")\n",
    "select\n",
    "    eventname,\n",
    "    count(1) as \"count\",\n",
    "    min(s3_write_delay_seconds) as \"s3_write_delay_seconds_min\",\n",
    "    avg(s3_write_delay_seconds) as \"s3_write_delay_seconds_avg\",\n",
    "    map_entries(numeric_histogram(100, s3_write_delay_seconds)) as \"s3_write_delay_seconds_histogram\",\n",
    "    avg(\"$file_size\") as size,\n",
    "    approx_percentile(s3_write_delay_seconds, 0.5) as \"s3_write_delay_seconds_p50\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.95) as \"s3_write_delay_seconds_p95\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.99) as \"s3_write_delay_seconds_p99\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.999) as \"s3_write_delay_seconds_p999\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.9999) as \"s3_write_delay_seconds_p9999\",\n",
    "    max(s3_write_delay_seconds) as \"s3_write_delay_seconds_max\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 60) as \"s3_write_delay_seconds_count_lt_60\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 300) as \"s3_write_delay_seconds_count_lt_300\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 600) as \"s3_write_delay_seconds_count_lt_600\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 3600) as \"s3_write_delay_seconds_count_lt_3600\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 86400) as \"s3_write_delay_seconds_count_lt_86400\"\n",
    "from\n",
    "    events\n",
    "where\n",
    "    eventdate between\n",
    "            format_datetime(current_date - interval '{1 + DAYS}' day, 'YYYY/MM/dd')\n",
    "        and format_datetime(current_date - interval '1' day, 'YYYY/MM/dd')\n",
    "    and eventname in ('AssumeRole', 'DescribeNetworkInterfaces', 'GetObject', 'PutObject')\n",
    "group by\n",
    "    eventname\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f269956-5188-4e8c-b9bc-89f1b09c8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_latency(\n",
    "    data_by_eventname.sort_values(\"eventname\"),\n",
    "    label_fn=lambda row: row[\"eventname\"],\n",
    "    title=\"CloudTrail delay statistics by event name\",\n",
    ").savefig(\"all_events_by_name.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80eccff-ee31-4a45-bec0-c03a4c14335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_sample = execute_query(f\"\"\"\n",
    "with events as (\n",
    "    select\n",
    "        *,\n",
    "        \"$path\",\n",
    "        parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as parsed_eventtime,\n",
    "        \"$file_modified_time\",\n",
    "        \"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as s3_write_delay_interval,\n",
    "        to_milliseconds(\"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ')) / 1000 as s3_write_delay_seconds,\n",
    "        regexp_extract(\"$path\", '\\d{8}T\\d{4}') as path_timestamp,\n",
    "       \"$file_size\"\n",
    "    from\n",
    "        \"{CLOUDTRAIL_ATHENA_TABLE_NAME}\"\n",
    "    where\n",
    "        addendum is null\n",
    "    and eventtype = 'AwsApiCall'\n",
    "),\n",
    "indexed_events_by_source as (\n",
    "    select\n",
    "        *,\n",
    "        row_number() over (partition by eventname order by random()) as index\n",
    "    from\n",
    "        events\n",
    "    where\n",
    "        eventdate between\n",
    "                format_datetime(current_date - interval '{1 + DAYS}' day, 'YYYY/MM/dd')\n",
    "            and format_datetime(current_date - interval '1' day, 'YYYY/MM/dd')\n",
    ")\n",
    "select\n",
    "    eventsource,\n",
    "    eventname,\n",
    "    eventcategory,\n",
    "    readonly,\n",
    "    parsed_eventtime,\n",
    "    s3_write_delay_seconds\n",
    "from\n",
    "    indexed_events_by_source\n",
    "where\n",
    "    index <= 10000\n",
    "    and eventname in ('AssumeRole', 'DescribeNetworkInterfaces', 'GetObject', 'PutObject')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3affc8d5-b732-4aba-b4a1-e56382a93b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_sample[\"eventtime_mod_5_min\"] = events_sample[\"parsed_eventtime\"].map(lambda t: (t.minute * 60 + t.second) % 300)\n",
    "draw_events(\n",
    "    events_sample.groupby(\"eventname\", sort=True)[[\"eventtime_mod_5_min\", \"s3_write_delay_seconds\"]],\n",
    "    title=\"CloudTrail delay by event name\",\n",
    "    xlabel=\"Event time (mod 5 minutes)\",\n",
    "    ylabel=\"CloudTrail delay (seconds)\",\n",
    ").savefig(\"sample_events_by_name.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de863752-56f1-4d4c-84d6-b7ea4846849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_accountregion = execute_query(f\"\"\"\n",
    "with events as (\n",
    "    select\n",
    "        *,\n",
    "        \"$path\",\n",
    "        parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as parsed_eventtime,\n",
    "        \"$file_modified_time\",\n",
    "        \"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as s3_write_delay_interval,\n",
    "        to_milliseconds(\"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ')) / 1000 as s3_write_delay_seconds,\n",
    "        regexp_extract(\"$path\", '\\d{8}T\\d{4}') as path_timestamp,\n",
    "       \"$file_size\"\n",
    "    from\n",
    "        \"{CLOUDTRAIL_ATHENA_TABLE_NAME}\"\n",
    "    where\n",
    "        addendum is null\n",
    "    and eventtype = 'AwsApiCall'\n",
    ")\n",
    "select\n",
    "    account,\n",
    "    region,\n",
    "    count(1) as \"count\",\n",
    "    min(s3_write_delay_seconds) as \"s3_write_delay_seconds_min\",\n",
    "    avg(s3_write_delay_seconds) as \"s3_write_delay_seconds_avg\",\n",
    "    map_entries(numeric_histogram(100, s3_write_delay_seconds)) as \"s3_write_delay_seconds_histogram\",\n",
    "    avg(\"$file_size\") as size,\n",
    "    approx_percentile(s3_write_delay_seconds, 0.5) as \"s3_write_delay_seconds_p50\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.95) as \"s3_write_delay_seconds_p95\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.99) as \"s3_write_delay_seconds_p99\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.999) as \"s3_write_delay_seconds_p999\",\n",
    "    approx_percentile(s3_write_delay_seconds, 0.9999) as \"s3_write_delay_seconds_p9999\",\n",
    "    max(s3_write_delay_seconds) as \"s3_write_delay_seconds_max\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 60) as \"s3_write_delay_seconds_count_lt_60\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 300) as \"s3_write_delay_seconds_count_lt_300\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 600) as \"s3_write_delay_seconds_count_lt_600\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 3600) as \"s3_write_delay_seconds_count_lt_3600\",\n",
    "    count(1) filter (where s3_write_delay_seconds <= 86400) as \"s3_write_delay_seconds_count_lt_86400\"\n",
    "from\n",
    "    events\n",
    "where\n",
    "    eventdate between\n",
    "            format_datetime(current_date - interval '{1 + DAYS}' day, 'YYYY/MM/dd')\n",
    "        and format_datetime(current_date - interval '0' day, 'YYYY/MM/dd')\n",
    "    and eventsource = 'sts.amazonaws.com'\n",
    "group by\n",
    "    account, region\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c4b65-1c66-4d9e-913d-f15091387e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_data_by_accountregion = data_by_accountregion.sort_values(\"count\", ascending = False)\n",
    "busy_account = ordered_data_by_accountregion.iloc[0][\"account\"]\n",
    "busy_account_regions = ordered_data_by_accountregion[ordered_data_by_accountregion[\"account\"] == busy_account]\n",
    "\n",
    "draw_latency(\n",
    "    busy_account_regions.iloc[[0, -1]],\n",
    "    label_fn=lambda row: \"busy region\" if row[\"region\"] == busy_account_regions.iloc[0][\"region\"] else \"quiet region\", # row[\"account\"] + \" \" + row[\"region\"]\n",
    "    title=\"CloudTrail delay statistics by region (single account)\",\n",
    ").savefig(\"busy_region_quiet_region.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b18498-ec41-486f-9978-41d1aab4acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_sample_delay = execute_query(f\"\"\"\n",
    "with events as (\n",
    "    select\n",
    "        *,\n",
    "        \"$path\",\n",
    "        parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as parsed_eventtime,\n",
    "        \"$file_modified_time\",\n",
    "        \"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ') as s3_write_delay_interval,\n",
    "        to_milliseconds(\"$file_modified_time\" - parse_datetime(eventtime, 'YYYY-MM-dd''T''HH:mm:ssZ')) / 1000 as s3_write_delay_seconds,\n",
    "        regexp_extract(\"$path\", '\\d{8}T\\d{4}') as path_timestamp,\n",
    "       \"$file_size\"\n",
    "    from\n",
    "        \"{CLOUDTRAIL_ATHENA_TABLE_NAME}\"\n",
    "    where\n",
    "        addendum is null\n",
    "    and eventtype = 'AwsApiCall'\n",
    "),\n",
    "indexed_events_by_source as (\n",
    "    select\n",
    "        *,\n",
    "        row_number() over (partition by account, region order by random()) as index,\n",
    "        to_milliseconds(parsed_eventtime - first_value(parsed_eventtime) over (partition by \"$path\" order by parsed_eventtime asc)) / 1000 as seconds_after_first,\n",
    "        count() over (partition by account, region, path_timestamp) as events_in_window\n",
    "    from\n",
    "        events\n",
    "    where\n",
    "        eventdate between\n",
    "                format_datetime(current_date - interval '{1 + DAYS}' day, 'YYYY/MM/dd')\n",
    "            and format_datetime(current_date - interval '1' day, 'YYYY/MM/dd')\n",
    ")\n",
    "select\n",
    "    eventsource,\n",
    "    eventname,\n",
    "    eventcategory,\n",
    "    readonly,\n",
    "    parsed_eventtime,\n",
    "    s3_write_delay_seconds,\n",
    "    seconds_after_first,\n",
    "    events_in_window\n",
    "from\n",
    "    indexed_events_by_source\n",
    "where\n",
    "    index <= 1000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8483232-d3f9-48de-a466-42f83868ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_events(\n",
    "    events_sample_delay.query(\"seconds_after_first < 600 and s3_write_delay_seconds < 600\").groupby(lambda a: False)[[\"seconds_after_first\", \"s3_write_delay_seconds\"]],\n",
    "    title=\"CloudTrail delay by time after first event in file\",\n",
    "    xlabel=\"Time after first event in file (seconds)\",\n",
    "    ylabel=\"CloudTrail delay (seconds)\",\n",
    ").savefig(\"events_sample_delay.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
